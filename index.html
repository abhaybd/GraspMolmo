<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="GraspMolmo is a generalizable open-vocabulary task-oriented grasping model that predicts semantically appropriate grasps given a natural language instruction.">
  <meta name="keywords" content="GraspMolmo, Grasping, Task-Oriented Grasping, Molmo, Synthetic Data, VLM, Robotics, Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GraspMolmo</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/sync_vids.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://abhaybd.github.io/">Abhay Deshpande</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/yuquand/">Yuquan Deng</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://arijitray1993.github.io/">Arijit Ray</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://jordisalvador-image.blogspot.com/">Jordi Salvador</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.semanticscholar.org/author/Winson-Han/1443358534">Winson Han</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://duanjiafei.com/">Jiafei Duan</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://kuohaozeng.github.io/">Kuo-Hao Zeng</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://ranjaykrishna.com/">Ranjay Krishna</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://rosehendrix.com/">Rose Hendrix</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>PRIOR @ Allen Institute for AI,</span>
            <span class="author-block"><sup>2</sup>Boston University,</span>
            <span class="author-block"><sup>3</sup>University of Washington,</span>
            <span class="author-block"><sup>4</sup>Vercept,</span>
            <span class="author-block"><sup>5</sup>UT Austin</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/abhaybd/GraspMolmo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/abhaybd/GraspMolmo/blob/main/DATA.md"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
              <!-- Checkpoint Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-archive"></i>
                  </span>
                  <span>Model Weights (Coming Soon)</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="teaser-video-container">
        <div class="teaser-video-row">
          <video muted class="synced synced-teaser">
            <source src="./static/videos/robot/bottle_lid.mp4" type="video/mp4">
          </video>
          <video muted class="synced synced-teaser">
            <source src="./static/videos/robot/frenchpress_pour.mp4" type="video/mp4">
          </video>
        </div>
        <div class="teaser-video-row">
          <video muted class="synced synced-teaser">
            <source src="./static/videos/robot/flowers.mp4" type="video/mp4">
          </video>
          <video muted class="synced synced-teaser">
            <source src="./static/videos/robot/phone_answer.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present GraspMolmo, a generalizable open-vocabulary task-oriented grasping (TOG) model, and PRISM,
            a large-scale synthetic dataset used to train it. GraspMolmo predicts
            semantically appropriate, stable grasps conditioned on a natural language instruction and a single RGB-D frame.
            For instance, given "pour me some tea", GraspMolmo selects a grasp on a teapot handle rather than its body or lid.
          </p>
          <p>
            Unlike prior TOG methods, which are limited by small datasets, simplistic language, and uncluttered scenes,
            GraspMolmo learns from PRISM, a novel large-scale synthetic dataset of 379k samples featuring cluttered environments and
            diverse, realistic task descriptions. We fine-tune the Molmo visual-language model on this data, enabling
            GraspMolmo to generalize to novel open-vocabulary instructions and objects.
          </p>
          <p>
            In challenging real-world evaluations, GraspMolmo achieves state-of-the-art results, with a 70% prediction
            success on complex tasks, compared to the 35% achieved by the next best alternative. GraspMolmo also
            successfully demonstrates the ability to predict semantically correct bimanual grasps zero-shot.
          </p>
          <p>
            We release our synthetic dataset, code, model, and benchmarks to accelerate research in task-semantic robotic
            manipulation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <img src="./static/images/teaser_transparent.png" alt="Overview of the PRISM dataset and GraspMolmo model">
    <h2 class="subtitle has-text-centered">
      GraspMolmo is a generalizable open-vocabulary task-oriented grasping model that predicts semantically appropriate grasps given a natural language instruction.
    </h2>
  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-2 has-text-centered">Data Generation</h2>

    <!-- Data Generation -->
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h3 class="title is-3">PRISM Dataset</h3>
          <div class="columns is-multiline is-centered">
            <div class="column is-full">
              <p>
                <b>P</b>urpose-driven <b>R</b>obotic <b>I</b>nteraction in <b>S</b>cene <b>M</b>anipulation (PRISM) is a large-scale synthetic dataset for Task-Oriented Grasping
                featuring cluttered environments and diverse, realistic task descriptions.
                We use 2365 object instances from ShapeNet-Sem along with stable grasps from ACRONYM to compose 10,000 unique and diverse scenes.
                Within each scene we capture 10 views, within which there are multiple tasks to be performed.
                This results in 379k task-grasp samples in total.
              </p>
            </div>
            <div class="column is-full">
              <img src="./static/images/datagen_overview.png" alt="Overview of the PRISM data generation pipeline">
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ Data Generation -->

    <!-- TaskGrasp-Image -->
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h3 class="title is-3">TaskGrasp-Image</h3>
          <div class="columns is-multiline is-centered">
            <div class="column is-full">
              <p>
                <a href="https://github.com/adithyamurali/TaskGrasp">TaskGrasp</a>
                is a standard benchmark for Task-Oriented Grasping, with manually annotated grasp annotations.
                However, TaskGrasp only contains segmented partial object point clouds, which (a) fails to capture the complexity of real-world scenes,
                and (b) includes fusion and segmentation artifacts. The use of point clouds also makes it difficult to leverage
                models that use RGB input, such as VLMs.
              </p>
              <p>
                To address these issues, we derive TaskGrasp-Image from TaskGrasp. TaskGrasp-Image consists of the same ground-truth annotations as TaskGrasp,
                but the grasp annotations have been transformed into the image frame, allowing models to use RGB input. We perform this transformation by
                using point-cloud registration techniques to align the point clouds with the segmented object points from each RGB-D frame.
                Since the image frames are, by construction, ground truth real data, TaskGrasp-Image does not suffer from fusion or segmentation artifacts.
                As a result, TaskGrasp-Image is a realistic and representative benchmark for Task-Oriented Grasping.
              </p>
            </div>
            <div class="column is-8">
              <img src="./static/images/taskgrasp_image_registration.png" alt="A visualization of the segmented pointcloud (red) of a pot registered to one of the input RGB-D views.">
              <p class="has-text-centered">
                A visualization of the segmented pointcloud (red) of a pot registered to one of the input RGB-D views.
                By performing this registration, we can transform the annotated grasps (yellow) into the camera frame.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ TaskGrasp-Image -->
  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-2 has-text-centered">Evaluation</h2>
    <!-- 
      Experimental Results
      - Table
      - Regression plot
      Real-world
      - Pictures of scenes
      - Full task table
      Bimanual
      - Videos
    -->
    <!-- Experimental Results -->
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h3 class="title is-3">Experimental Results</h3>
            <div class="columns is-multiline is-centered">
              <div class="column is-full">
                <p>
                  We evaluate GraspMolmo and the most appropriate baseline on three distinct benchmark settings, progressively increasing in complexity and real-world applicability: 
                  a benchmark from literature with simple objects and minimal visual diversity, a synthetic held-out dataset of fully composed scenes with unseen objects, and finally, 
                  real-world transfer scenarios. The performance gap between methods widens notably as we progress from simpler to more complex evaluation scenarios, revealing
                  fundamental differences in approach capabilities that are not apparent in basic benchmarks.
                </p>
                <p>
                  We see that GraspMolmo outperforms all baselines in all evaluations, especially in real-world transfer experiments. This demonstrates its powerful generalization capabilities,
                  and its ability to handle complexity in scene composition, object diversity, and task specification.
                </p>
                <p>
                  We also note that performance on PRISM-Test correlates well with real-world performance,
                  further demonstrating its importance and utility.
                </p>
              </div>
              <div class="column is-6">
                <img src="static/images/correlations.png" alt="PRISM-Test is a better predictor of real-world performance than TaskGrasp-Image.">
                <p class="has-text-centered">
                  PRISM-Test is a better predictor of real-world performance than TaskGrasp-Image.
                </p>
              </div>
              <div class="column is-full">
                <div class="table-container has-text-centered">
                <table class="results-table">
                  <tr>
                    <th></th>
                    <th>TaskGrasp-Image</th>
                    <th>PRISM-Test</th>
                    <th>PRISM-Real (Prediction)</th>
                    <th>PRISM-Real (Overall)</th>
                  </tr>
                  <tr>
                    <td>Random</td>
                    <td>54.5%</td>
                    <td>29.3%</td>
                    <td>-</td>
                    <td>-</td>
                  </tr>
                  <tr>
                    <td>GraspGPT</td>
                    <td>72.3%</td>
                    <td>40.0%</td>
                    <td>35.1%</td>
                    <td>24.0%</td>
                  </tr>
                  <tr>
                    <td>Molmo</td>
                    <td>75.6%</td>
                    <td>49.8%</td>
                    <td>33.7%</td>
                    <td>31.0%</td>
                  </tr>
                  <tr>
                    <td><b>GraspMolmo</b></td>
                    <td><b>76.7%</b></td>
                    <td><b>62.5%</b></td>
                    <td><b>70.4%</b></td>
                    <td><b>61.1%</b></td>
                  </tr>
                </table>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ Experimental Results -->

    <!-- Real-World Evaluation -->
    <h3 class="title is-3">Real-World Evaluation</h3>
    <div class="columns is-centered is-multiline">
      <div class="column is-full">
        <div class="columns is-centered">
          <div class="column is-4">
            <img src="static/images/scenes/scene1.jpg" alt="Scene 1">
            <p class="has-text-centered">Scene 1</p>
          </div>
          <div class="column is-4">
            <img src="static/images/scenes/scene2.jpg" alt="Scene 2">
            <p class="has-text-centered">Scene 2</p>
          </div>
          <div class="column is-4">
            <img src="static/images/scenes/scene3.jpg" alt="Scene 3">
            <p class="has-text-centered">Scene 3</p>
          </div>
        </div>
      </div>
      <div class="column is-9">
        <div class="content">
          <div class="table-container has-text-centered">
            <table class="scenes-table">
              <tr>
                <td rowspan="6">Scene 1</td>
                <td rowspan="2">French Press</td>
                <td>"Pour coffee from the french press"</td>
              </tr>
              <tr>
                <td>"Press down the knob of the plunger of the french press"</td>
              </tr>
              <tr>
                <td rowspan="2">Kitchen Knife</td>
                <td>"Use the knife to cut fruit"</td>
              </tr>
              <tr>
                <td>"Hand me the knife safely"</td>
              </tr>
              <tr>
                <td rowspan="2">Mug</td>
                <td>"Pour the water out of the blue mug"</td>
              </tr>
              <tr>
                <td>"Hang the blue mug onto a hook by the handle"</td>
              </tr>
              <tr>
                <td rowspan="6">Scene 2</td>
                <td rowspan="2">Water Bottle</td>
                <td>"Open the lid of the water bottle"</td>
              </tr>
              <tr>
                <td>"Give me some water"</td>
              </tr>
              <tr>
                <td rowspan="2">Sink</td>
                <td>"Adjust the faucet"</td>
              </tr>
              <tr>
                <td>"Turn on the sink"</td>
              </tr>
              <tr>
                <td rowspan="2">Spray Bottle</td>
                <td>"Spray cleaning solution with the spray bottle"</td>
              </tr>
              <tr>
                <td>"Unscrew the spray bottle"</td>
              </tr>
              <tr>
                <td rowspan="6">Scene 3</td>
                <td rowspan="2">Books</td>
                <td>"Pass the book written by Greg Bear"</td>
              </tr>
              <tr>
                <td>"Pass the book written by Orson Scott Card"</td>
              </tr>
              <tr>
                <td rowspan="2">Telephone</td>
                <td>"Answer the phone"</td>
              </tr>
              <tr>
                <td>"Put the phone back on the hook"</td>
              </tr>
              <tr>
                <td rowspan="2">Flower + Vase</td>
                <td>"Take the flowers out of the vase"</td>
              </tr>
              <tr>
                <td>"Dump the flowers out of the vase"</td>
              </tr>
            </table>            
          </div>
          <p class="has-text-centered">
            A full list of the scenes, objects, and tasks used for real-world evaluation.
          </p>
        </div>
      </div>
    </div>

    <!-- Bimanual -->
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h3 class="title is-3">Extension to Bimanual Grasping</h3>
          <div class="columns is-multiline is-centered">
            <div class="column is-6">
              <video muted class="synced synced-bimanual">
                <source src="./static/videos/robot/bottle.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-6">
              <video muted class="synced synced-bimanual">
                <source src="./static/videos/robot/box.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-10">
              <p class="has-text-centered">
                We also see that by performing multiple predictions, GraspMolmo can predict multiple grasps, enabling
                complex task-oriented bimanual grasping. To do so, we decompose a bimanual task into two single-arm tasks,
                and use GraspMolmo to predict the grasps for each arm sequentially. For example, "Open the water bottle" becomes "Lift the water bottle" and "Unscrew the lid".
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ Bimanual -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{deshpande2025graspmolmo,
  author    = {Deshpande, Abhay and Deng, Yuquan and Ray, Arijit and Salvador, Jordi and Han, Winson and Duan, Jiafei and Zeng, Kuo-Hao and Zhu, Yuke and Krishna, Ranjay and Hendrix, Rose},
  title     = {GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation},
  journal   = {arXiv preprint arXiv:XXXXX},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website template taken from <a href="https://nerfies.github.io/">NeRFies</a>, by Keunhong Park.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
