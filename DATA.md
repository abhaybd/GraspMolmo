# PRISM

The PRISM dataset is available for download:
- [PRISM-Train](https://pub-3e61ad92c7024712b84e4bf8658147f7.r2.dev/PRISM_train.tar)
- [PRISM-Test](https://pub-3e61ad92c7024712b84e4bf8658147f7.r2.dev/PRISM_test.tar)
- [Splits](https://pub-3e61ad92c7024712b84e4bf8658147f7.r2.dev/PRISM_splits.tar) (train/test split of each object instance in each class)

## Data Overview

Each of the above splits contains:
- Procedurally generated 3D scenes (output of `datagen.py`), containing geometry, arrangement, grasp, view pose, and lighting information.
- Tasks matched with grasps in the viewpoints of the generated scenes

### Pregenerated scenes

The scenes data in each split is structured as follows:
```
<scene_id>
  scene.pkl (contains all scene information)
  metadata.yaml (contains list of graspable objects in scene)
```

Each `scene.pkl` contains a dictionary with the following structure:
```
{
    "annotations": {
        <annot_id>: [Annotation object, 4x4 grasp pose in world frame, 3D point in world frame being grasped]
    },
    "lighting": {
        [
            {
                "type": "DirectionalLight",
                "args": {
                    "name": "light",
                    "color": [r, g, b] # 0-255
                    "intensity": float
                },
                "transform": 4x4 pose matrix of light
            },
            ...
        ]
    },
    "views": [
        {
            "cam_K": 3x3 camera intrinsics matrix,
            "cam_pose": 4x4 camera pose matrix (note that the camera z-axis points backwards),
            "annotations_in_view": list[str] of <annot_ids> in annotations dict which are visible from this view
        },
        ...
    ]
    "glb": base64-encoded GLB string of 3D scene,
    "img_size": (height, width)
}
```

### Rendering observations

Notably, this data does not include the rendered views, for the purposes of space-saving. These views can be rendered using `generate_obs.py`, which will generate downstream information from the scene data. Each scene will result in a single `<scene_id>.hdf5` file, with the following structure:

```
view_<i>/
    rgb: RGB image as (H,W,3) array
    xyz: Back-projected point-cloud (from RGB-D view) as (H,W,3) array of XYZ points
    seg: Segmentation map as (H,W) array where each pixel is index of object name in object_names
    object_names: List of object names visible in view
    normals (optional): Point-cloud normals as (H,W,3) array
    view_pose: Camera pose in world frame as (4,4) array
    cam_params: Camera intrinsics matrix as (3,3) array

    obs_<j>/
        grasp_pose: Grasp pose in camera frame as (4,4) array
        grasp_point: Point being grasped in camera frame as (3,) array
        grasp_point_px: Point being grasped projected onto image plane as (2,) array
        annot: YAML-formatted object with the following keys: ["annotation_id", "grasp_description", "object_description", "object_category", "object_id", "grasp_id"]
```

### Pregenerated Task-Grasps

The tasks and matching grasps are pregenerated and included in the data download. This is in the form of a CSV file with the following columns:
```
scene_path: The file name of the scene .hdf5 file generated by generate_obs.py
scene_id: The scene ID
view_id: The view ID, i.e. the key in the scene HDF5
obs_id: The observation ID, i.e. the key in the view group in the scene HDF5
task: The natural language task description
original_grasp_desc: The original generated matching grasp description, included for posterity
matching_grasp_desc: The matched grasp description, which is the same as <view_id>/<obs_id>/annot["grasp_description"] in the scene HDF5.
```

The rows of this CSV are the samples of the dataset.

# TaskGrasp-Image

The TaskGrasp-Image dataset is available to download [here](#).
